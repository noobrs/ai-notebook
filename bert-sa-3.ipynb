{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50cffd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/Train.csv\")\n",
    "df_test = pd.read_csv(\"datasets/Test.csv\")\n",
    "df_valid = pd.read_csv(\"datasets/Valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e15d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, emoji, html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text(\" \")  # strip HTML tags\n",
    "    text = html.unescape(text)                               # e.g. &amp; ‚Üí &\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))       # üòÄ ‚Üí  smile_face \n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)                          # remove mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)                          # remove hashtags\n",
    "    text = re.sub(r\"[^\\w\\s.,!?;:]\", \"\", text)       # remove special characters\n",
    "    text = text.lower()                                       # convert to lowercase    \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()                 # collapse spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717ab146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ai-notebook\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b32ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class sentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text  = self.texts[idx]\n",
    "        label = int(self.labels[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3376d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_raw = df[\"text\"].astype(str).tolist()\n",
    "train_texts = [clean(t) for t in train_texts_raw]\n",
    "train_labels = df[\"label\"].tolist()\n",
    "\n",
    "test_texts_raw = df_test[\"text\"].astype(str).tolist()\n",
    "test_texts = [clean(t) for t in test_texts_raw]\n",
    "test_labels = df_test[\"label\"].tolist()\n",
    "\n",
    "valid_texts_raw = df_valid[\"text\"].astype(str).tolist()\n",
    "valid_texts = [clean(t) for t in valid_texts_raw]\n",
    "valid_labels = df_valid[\"label\"].tolist()\n",
    "\n",
    "train_dataset = sentimentDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset  = sentimentDataset(test_texts,  test_labels,  tokenizer)\n",
    "valid_dataset = sentimentDataset(valid_texts, valid_labels, tokenizer)\n",
    "\n",
    "# (optional) Slightly faster host‚ÜíGPU copies\n",
    "cuda = torch.cuda.is_available()\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,  pin_memory=cuda)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, pin_memory=cuda)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=16, shuffle=False, pin_memory=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376e1995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [29:34<00:00,  1.41it/s]\n",
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.2447 | val_loss=0.1716 | val_acc=0.9358 | val_f1=0.9357\n",
      "‚úì Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [34:28<00:00,  1.21it/s]\n",
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss=0.1088 | val_loss=0.1686 | val_acc=0.9424 | val_f1=0.9424\n",
      "‚úì Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2500/2500 [33:55<00:00,  1.23it/s]\n",
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss=0.0413 | val_loss=0.1985 | val_acc=0.9448 | val_f1=0.9448\n",
      "‚úì Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST METRICS ===\n",
      "{'loss': 0.1837186083054771, 'accuracy': 0.9474, 'precision': 0.947521024112429, 'recall': 0.9473841895367582, 'f1': 0.9473949478107877}\n",
      "\n",
      "=== CLASSIFICATION REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9544    0.9395    0.9469      2495\n",
      "           1     0.9406    0.9553    0.9479      2505\n",
      "\n",
      "    accuracy                         0.9474      5000\n",
      "   macro avg     0.9475    0.9474    0.9474      5000\n",
      "weighted avg     0.9475    0.9474    0.9474      5000\n",
      "\n",
      "\n",
      "=== CONFUSION MATRIX ===\n",
      "[[2344  151]\n",
      " [ 112 2393]]\n"
     ]
    }
   ],
   "source": [
    "# --- TRAIN & EVAL (drop-in) -----------------------------------------------\n",
    "import os, random, numpy as np, torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "epochs = 3                    # bump to 4‚Äì5 if you have time\n",
    "lr = 2e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Mixed precision\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "amp_enabled = (device_type == \"cuda\")\n",
    "scaler = torch.amp.GradScaler(enabled=amp_enabled)\n",
    "\n",
    "# ----------------- Evaluation helper -----------------\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
    "            # to device\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "            # forward\n",
    "            with torch.amp.autocast(device_type=device_type, enabled=amp_enabled):\n",
    "                outputs = model(**batch)  # expects input_ids, attention_mask, labels\n",
    "                loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = outputs.logits.argmax(dim=-1).detach().cpu().numpy()\n",
    "            labels = batch[\"labels\"].detach().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(dataloader))\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "    return {\"loss\": avg_loss, \"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}, np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# ----------------- Training loop -----------------\n",
    "best_f1 = 0.0\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n",
    "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device_type, enabled=amp_enabled):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / max(1, len(train_loader))\n",
    "    val_metrics, y_true_val, y_pred_val = evaluate(valid_loader)\n",
    "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f} | \"\n",
    "          f\"val_loss={val_metrics['loss']:.4f} | \"\n",
    "          f\"val_acc={val_metrics['accuracy']:.4f} | \"\n",
    "          f\"val_f1={val_metrics['f1']:.4f}\")\n",
    "\n",
    "    # Save best by macro-F1\n",
    "    if val_metrics[\"f1\"] > best_f1:\n",
    "        best_f1 = val_metrics[\"f1\"]\n",
    "        torch.save(model.state_dict(), \"checkpoints/bert_sentiment_best.pt\")\n",
    "        print(\"‚úì Saved new best model\")\n",
    "\n",
    "# ----------------- Final test -----------------\n",
    "# load best\n",
    "model.load_state_dict(torch.load(\"checkpoints/bert_sentiment_best.pt\", map_location=device))\n",
    "test_metrics, y_true, y_pred = evaluate(test_loader)\n",
    "print(\"\\n=== TEST METRICS ===\")\n",
    "print(test_metrics)\n",
    "\n",
    "print(\"\\n=== CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "print(\"\\n=== CONFUSION MATRIX ===\")\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aed21e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review pred_label  \\\n",
      "0  This was painfully boring. I almost fell aslee...   negative   \n",
      "1    Terrible plot but the cinematography is decent.   negative   \n",
      "2  It's okay overall‚Äînothing special, nothing awful.   positive   \n",
      "3  I really enjoyed the characters, solid pacing ...   positive   \n",
      "4         Masterpiece. Best film I've seen in years.   positive   \n",
      "5  Mediocre at best; some scenes work but most do...   negative   \n",
      "6  Awful acting, clumsy script. Would not recommend.   negative   \n",
      "7   Charming and heartfelt with a satisfying ending.   positive   \n",
      "\n",
      "   pred_confidence  \n",
      "0         0.997285  \n",
      "1         0.989064  \n",
      "2         0.844082  \n",
      "3         0.999103  \n",
      "4         0.989832  \n",
      "5         0.995147  \n",
      "6         0.998520  \n",
      "7         0.998122  \n",
      "\n",
      "Review: This was painfully boring. I almost fell asleep halfway through.\n",
      "         negative: 0.9973\n",
      "         positive: 0.0027\n",
      "\n",
      "Review: Terrible plot but the cinematography is decent.\n",
      "         negative: 0.9891\n",
      "         positive: 0.0109\n",
      "\n",
      "Review: It's okay overall‚Äînothing special, nothing awful.\n",
      "         negative: 0.1559\n",
      "         positive: 0.8441\n",
      "\n",
      "Review: I really enjoyed the characters, solid pacing and great music!\n",
      "         negative: 0.0009\n",
      "         positive: 0.9991\n",
      "\n",
      "Review: Masterpiece. Best film I've seen in years.\n",
      "         negative: 0.0102\n",
      "         positive: 0.9898\n",
      "\n",
      "Review: Mediocre at best; some scenes work but most don't.\n",
      "         negative: 0.9951\n",
      "         positive: 0.0049\n",
      "\n",
      "Review: Awful acting, clumsy script. Would not recommend.\n",
      "         negative: 0.9985\n",
      "         positive: 0.0015\n",
      "\n",
      "Review: Charming and heartfelt with a satisfying ending.\n",
      "         negative: 0.0019\n",
      "         positive: 0.9981\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import re, emoji, html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Config: labels & checkpoint\n",
    "# -----------------------------\n",
    "# Edit this to your classes. Order must match your training label IDs (0..K-1)\n",
    "# Example for 5 classes:\n",
    "label_names = [\"negative\", \"positive\"]\n",
    "# Example for 3 classes (uncomment instead):\n",
    "# label_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "NUM_LABELS = len(label_names)\n",
    "checkpoint_path = \"checkpoints/bert_sentiment_best.pt\"   # change if different\n",
    "pretrained_name = \"bert-base-uncased\"                    # must match training\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Same cleaner you used before\n",
    "# -----------------------------\n",
    "def clean(text: str) -> str:\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text(\" \")  # strip HTML tags\n",
    "    text = html.unescape(text)                               # e.g. &amp; ‚Üí &\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))       # üòÄ ‚Üí  smile_face \n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)                         # mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)                         # hashtags\n",
    "    text = re.sub(r\"[^\\w\\s.,!?;:]\", \"\", text)                # special chars\n",
    "    text = text.lower()                                      # lowercase\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()                 # collapse spaces\n",
    "    return text\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Create some example reviews\n",
    "# -----------------------------\n",
    "example_reviews = [\n",
    "    \"This was painfully boring. I almost fell asleep halfway through.\",\n",
    "    \"Terrible plot but the cinematography is decent.\",\n",
    "    \"It's okay overall‚Äînothing special, nothing awful.\",\n",
    "    \"I really enjoyed the characters, solid pacing and great music!\",\n",
    "    \"Masterpiece. Best film I've seen in years.\",\n",
    "    \"Mediocre at best; some scenes work but most don't.\",\n",
    "    \"Awful acting, clumsy script. Would not recommend.\",\n",
    "    \"Charming and heartfelt with a satisfying ending.\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Load tokenizer & model\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_name)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    pretrained_name,\n",
    "    num_labels=NUM_LABELS\n",
    ")\n",
    "# Load your fine-tuned weights\n",
    "state = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Prediction helpers\n",
    "# -----------------------------\n",
    "def predict_reviews(reviews, max_length=512, batch_size=16):\n",
    "    # clean\n",
    "    texts = [clean(str(t)) for t in reviews]\n",
    "\n",
    "    all_preds, all_probs = [], []\n",
    "    # simple batching for large lists\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            preds = probs.argmax(dim=-1)\n",
    "\n",
    "        all_preds.extend(preds.detach().cpu().tolist())\n",
    "        all_probs.extend(probs.detach().cpu().tolist())\n",
    "\n",
    "    # build a results dataframe\n",
    "    top_prob = [max(p) for p in all_probs]\n",
    "    pred_label = [label_names[i] for i in all_preds]\n",
    "    df = pd.DataFrame({\n",
    "        \"review\": reviews,\n",
    "        \"review_clean\": texts,\n",
    "        \"pred_label\": pred_label,\n",
    "        \"pred_id\": all_preds,\n",
    "        \"pred_confidence\": top_prob\n",
    "    })\n",
    "    return df, all_probs\n",
    "\n",
    "def predict_one(review_text):\n",
    "    df, _ = predict_reviews([review_text])\n",
    "    row = df.iloc[0]\n",
    "    return row[\"pred_label\"], row[\"pred_confidence\"], row\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Run on the sample reviews\n",
    "# -----------------------------\n",
    "results_df, probs = predict_reviews(example_reviews)\n",
    "print(results_df[[\"review\", \"pred_label\", \"pred_confidence\"]])\n",
    "\n",
    "# (Optional) show full probability distribution per review\n",
    "for i, r in enumerate(example_reviews):\n",
    "    print(f\"\\nReview: {r}\")\n",
    "    for k, p in zip(label_names, probs[i]):\n",
    "        print(f\"  {k:>15}: {p:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7) How to use with your own text(s)\n",
    "# -----------------------------\n",
    "# Example:\n",
    "# label, conf, row = predict_one(\"The movie was outstanding, I loved every minute!\")\n",
    "# print(label, conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9add2a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review pred_label  \\\n",
      "0  I‚Äôm thrilled to report that this film redefine...   positive   \n",
      "1  On paper, this should have been my favorite re...   negative   \n",
      "2  I didn‚Äôt exactly dislike it, which isn‚Äôt to sa...   negative   \n",
      "3  Wow, what an experience ü§Ø‚ÄîI learned exactly ho...   positive   \n",
      "4  If mediocrity were an art, this would hang in ...   negative   \n",
      "\n",
      "   pred_confidence  \n",
      "0         0.988354  \n",
      "1         0.997311  \n",
      "2         0.670189  \n",
      "3         0.896984  \n",
      "4         0.995633  \n"
     ]
    }
   ],
   "source": [
    "trick_reviews = [\n",
    "    r\"I‚Äôm thrilled to report that this film redefined cinema for me‚Äîspecifically, it redefined the precise moment I should have left the theater. The opening shot is breathtaking if your breath is taken by beige hallways and unlit rooms. Performances are ‚Äúcommitted,‚Äù in the sense that the actors should be committed for agreeing to this script. The director clearly loves long pauses; unfortunately, so did my streaming app when I tried to fast-forward. The plot twist arrives like a surprise party thrown for someone who moved away last year: nobody asked for it and the cake is stale. Still, the soundtrack works‚Äîif your playlist is ‚ÄúElevator Drones Vol. 3.‚Äù Credit where due: the credits rolled eventually. I can‚Äôt wait to recommend this to people I don‚Äôt like, so we have something new not to talk about.\",\n",
    "    r\"On paper, this should have been my favorite release of the year. The lighting is meticulous, the production design has that lived-in feel, and a few lines sparkle with wit. Yet scene after scene drifts by like a well-polished boat going nowhere. I admired the craft while checking the time; I enjoyed the score while wondering why the characters sounded like they were reading stage directions. The lead‚Äôs vulnerability is disarming, but it mostly disarms tension. Even the climax feels carefully underlined in pencil‚Äîtechnically clear, emotionally faint. I left thinking, ‚ÄúThat was‚Ä¶ competent?‚Äù I wouldn‚Äôt call it bad; I‚Äôd call it an elegantly framed shrug, the kind of movie you recommend with a soft maybe and a strong caveat. If you need something pretty for a lazy Sunday, it‚Äôs fine. If you want to feel anything sharper than polite approval, you may want to look elsewhere.\",\n",
    "    r\"I didn‚Äôt exactly dislike it, which isn‚Äôt to say I liked it‚Äîmore that I can‚Äôt claim I wasn‚Äôt not entertained. The pacing isn‚Äôt entirely without momentum, and the jokes aren‚Äôt completely devoid of timing, though they rarely land in a way that would be impossible to ignore. The performances don‚Äôt lack effort, but the effort doesn‚Äôt quite not show, if you follow. It‚Äôs not the kind of movie I wouldn‚Äôt avoid recommending to someone who‚Äôs not uninterested in background noise. By the time the not-unexpected ending arrived, I was neither unmoved nor moved, just not unready for the credits. In fairness, the cinematography isn‚Äôt unappealing, and the score isn‚Äôt unlistenable; both do a job that isn‚Äôt unneeded. I suppose calling it ‚Äúnot terrible‚Äù wouldn‚Äôt be inaccurate, as long as ‚Äúnot good‚Äù doesn‚Äôt feel unkind. If ambiguity was the goal, mission‚Ä¶ not unaccomplished.\",\n",
    "    r\"Wow, what an experience ü§Ø‚ÄîI learned exactly how long ninety minutes can feel when time stops. The trailer promised thrills üòç, but the movie delivered responsible seatbelt usage and a deep respect for indoor voices. The romance is ‚Äúsimmering‚Äù if by simmering you mean two strangers who occasionally nod. I loved the soundtrack in the sense that silence is technically a sound üëÇ. Credit to the editor for keeping every scene long enough to wonder if my remote died. There‚Äôs a twist ending üéÅ, which is adorable‚Äîlike when a toddler hides behind a curtain and says, ‚ÄúGuess where I am!‚Äù Still, I‚Äôm grateful: the closing credits gave me the year‚Äôs most honest character arc‚Äîme, leaving. If you crave excitement, try watching paint dry, then watch this to cool down. Five stars for existing ‚≠ê (calm down, algorithm, that‚Äôs sarcasm), zero for joy.\",\n",
    "    r\"If mediocrity were an art, this would hang in a tasteful gallery next to a plaque that reads, ‚ÄúAdequate, 2025.‚Äù Compared to the director‚Äôs earlier disaster, it‚Äôs practically a comeback; compared to anything genuinely good, it‚Äôs a carefully ironed bedsheet‚Äîsmooth, flat, and forgettable. The performances are fine in that way airplanes are fine: you arrive, you don‚Äôt applaud. I admired how the script avoids clich√©s by replacing them with footnotes, each scene explaining why it exists instead of earning the right to. Yet I can‚Äôt deny a strange comfort: it‚Äôs competent, even considerate, never insulting my intelligence so much as politely ignoring it. Tomorrow I‚Äôll remember a line or two; next week I‚Äôll struggle to recall the title. Would I watch it again? Only if I‚Äôve already seen everything else and need something to not mind missing.\"\n",
    "    ]\n",
    "results_df, probs = predict_reviews(trick_reviews)\n",
    "print(results_df[[\"review\", \"pred_label\", \"pred_confidence\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
