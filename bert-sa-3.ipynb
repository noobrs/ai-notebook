{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50cffd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/Train.csv\")\n",
    "df_test = pd.read_csv(\"datasets/Test.csv\")\n",
    "df_valid = pd.read_csv(\"datasets/Valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e15d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, emoji, html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean(text):\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text(\" \")  # strip HTML tags\n",
    "    text = html.unescape(text)                               # e.g. &amp; → &\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))       # 😀 →  smile_face \n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)                          # remove mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)                          # remove hashtags\n",
    "    text = re.sub(r\"[^\\w\\s.,!?;:]\", \"\", text)       # remove special characters\n",
    "    text = text.lower()                                       # convert to lowercase    \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()                 # collapse spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717ab146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ai-notebook\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b32ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class sentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text  = self.texts[idx]\n",
    "        label = int(self.labels[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3376d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_raw = df[\"text\"].astype(str).tolist()\n",
    "train_texts = [clean(t) for t in train_texts_raw]\n",
    "train_labels = df[\"label\"].tolist()\n",
    "\n",
    "test_texts_raw = df_test[\"text\"].astype(str).tolist()\n",
    "test_texts = [clean(t) for t in test_texts_raw]\n",
    "test_labels = df_test[\"label\"].tolist()\n",
    "\n",
    "valid_texts_raw = df_valid[\"text\"].astype(str).tolist()\n",
    "valid_texts = [clean(t) for t in valid_texts_raw]\n",
    "valid_labels = df_valid[\"label\"].tolist()\n",
    "\n",
    "train_dataset = sentimentDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset  = sentimentDataset(test_texts,  test_labels,  tokenizer)\n",
    "valid_dataset = sentimentDataset(valid_texts, valid_labels, tokenizer)\n",
    "\n",
    "# (optional) Slightly faster host→GPU copies\n",
    "cuda = torch.cuda.is_available()\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,  pin_memory=cuda)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, pin_memory=cuda)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=16, shuffle=False, pin_memory=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376e1995",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 2500/2500 [29:34<00:00,  1.41it/s]\n",
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.2447 | val_loss=0.1716 | val_acc=0.9358 | val_f1=0.9357\n",
      "✓ Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 2500/2500 [34:28<00:00,  1.21it/s]\n",
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_loss=0.1088 | val_loss=0.1686 | val_acc=0.9424 | val_f1=0.9424\n",
      "✓ Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 2500/2500 [33:55<00:00,  1.23it/s]\n",
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_loss=0.0413 | val_loss=0.1985 | val_acc=0.9448 | val_f1=0.9448\n",
      "✓ Saved new best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST METRICS ===\n",
      "{'loss': 0.1837186083054771, 'accuracy': 0.9474, 'precision': 0.947521024112429, 'recall': 0.9473841895367582, 'f1': 0.9473949478107877}\n",
      "\n",
      "=== CLASSIFICATION REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9544    0.9395    0.9469      2495\n",
      "           1     0.9406    0.9553    0.9479      2505\n",
      "\n",
      "    accuracy                         0.9474      5000\n",
      "   macro avg     0.9475    0.9474    0.9474      5000\n",
      "weighted avg     0.9475    0.9474    0.9474      5000\n",
      "\n",
      "\n",
      "=== CONFUSION MATRIX ===\n",
      "[[2344  151]\n",
      " [ 112 2393]]\n"
     ]
    }
   ],
   "source": [
    "# --- TRAIN & EVAL (drop-in) -----------------------------------------------\n",
    "import os, random, numpy as np, torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "epochs = 3                    # bump to 4–5 if you have time\n",
    "lr = 2e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Mixed precision\n",
    "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "amp_enabled = (device_type == \"cuda\")\n",
    "scaler = torch.amp.GradScaler(enabled=amp_enabled)\n",
    "\n",
    "# ----------------- Evaluation helper -----------------\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Eval\", leave=False):\n",
    "            # to device\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "            # forward\n",
    "            with torch.amp.autocast(device_type=device_type, enabled=amp_enabled):\n",
    "                outputs = model(**batch)  # expects input_ids, attention_mask, labels\n",
    "                loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = outputs.logits.argmax(dim=-1).detach().cpu().numpy()\n",
    "            labels = batch[\"labels\"].detach().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(dataloader))\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "    return {\"loss\": avg_loss, \"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}, np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "# ----------------- Training loop -----------------\n",
    "best_f1 = 0.0\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n",
    "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device_type, enabled=amp_enabled):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_loss = running_loss / max(1, len(train_loader))\n",
    "    val_metrics, y_true_val, y_pred_val = evaluate(valid_loader)\n",
    "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f} | \"\n",
    "          f\"val_loss={val_metrics['loss']:.4f} | \"\n",
    "          f\"val_acc={val_metrics['accuracy']:.4f} | \"\n",
    "          f\"val_f1={val_metrics['f1']:.4f}\")\n",
    "\n",
    "    # Save best by macro-F1\n",
    "    if val_metrics[\"f1\"] > best_f1:\n",
    "        best_f1 = val_metrics[\"f1\"]\n",
    "        torch.save(model.state_dict(), \"checkpoints/bert_sentiment_best.pt\")\n",
    "        print(\"✓ Saved new best model\")\n",
    "\n",
    "# ----------------- Final test -----------------\n",
    "# load best\n",
    "model.load_state_dict(torch.load(\"checkpoints/bert_sentiment_best.pt\", map_location=device))\n",
    "test_metrics, y_true, y_pred = evaluate(test_loader)\n",
    "print(\"\\n=== TEST METRICS ===\")\n",
    "print(test_metrics)\n",
    "\n",
    "print(\"\\n=== CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "print(\"\\n=== CONFUSION MATRIX ===\")\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aed21e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review pred_label  \\\n",
      "0  This was painfully boring. I almost fell aslee...   negative   \n",
      "1    Terrible plot but the cinematography is decent.   negative   \n",
      "2  It's okay overall—nothing special, nothing awful.   positive   \n",
      "3  I really enjoyed the characters, solid pacing ...   positive   \n",
      "4         Masterpiece. Best film I've seen in years.   positive   \n",
      "5  Mediocre at best; some scenes work but most do...   negative   \n",
      "6  Awful acting, clumsy script. Would not recommend.   negative   \n",
      "7   Charming and heartfelt with a satisfying ending.   positive   \n",
      "\n",
      "   pred_confidence  \n",
      "0         0.997285  \n",
      "1         0.989064  \n",
      "2         0.844082  \n",
      "3         0.999103  \n",
      "4         0.989832  \n",
      "5         0.995147  \n",
      "6         0.998520  \n",
      "7         0.998122  \n",
      "\n",
      "Review: This was painfully boring. I almost fell asleep halfway through.\n",
      "         negative: 0.9973\n",
      "         positive: 0.0027\n",
      "\n",
      "Review: Terrible plot but the cinematography is decent.\n",
      "         negative: 0.9891\n",
      "         positive: 0.0109\n",
      "\n",
      "Review: It's okay overall—nothing special, nothing awful.\n",
      "         negative: 0.1559\n",
      "         positive: 0.8441\n",
      "\n",
      "Review: I really enjoyed the characters, solid pacing and great music!\n",
      "         negative: 0.0009\n",
      "         positive: 0.9991\n",
      "\n",
      "Review: Masterpiece. Best film I've seen in years.\n",
      "         negative: 0.0102\n",
      "         positive: 0.9898\n",
      "\n",
      "Review: Mediocre at best; some scenes work but most don't.\n",
      "         negative: 0.9951\n",
      "         positive: 0.0049\n",
      "\n",
      "Review: Awful acting, clumsy script. Would not recommend.\n",
      "         negative: 0.9985\n",
      "         positive: 0.0015\n",
      "\n",
      "Review: Charming and heartfelt with a satisfying ending.\n",
      "         negative: 0.0019\n",
      "         positive: 0.9981\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import re, emoji, html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Config: labels & checkpoint\n",
    "# -----------------------------\n",
    "# Edit this to your classes. Order must match your training label IDs (0..K-1)\n",
    "# Example for 5 classes:\n",
    "label_names = [\"negative\", \"positive\"]\n",
    "# Example for 3 classes (uncomment instead):\n",
    "# label_names = [\"negative\", \"neutral\", \"positive\"]\n",
    "\n",
    "NUM_LABELS = len(label_names)\n",
    "checkpoint_path = \"checkpoints/bert_sentiment_best.pt\"   # change if different\n",
    "pretrained_name = \"bert-base-uncased\"                    # must match training\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Same cleaner you used before\n",
    "# -----------------------------\n",
    "def clean(text: str) -> str:\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text(\" \")  # strip HTML tags\n",
    "    text = html.unescape(text)                               # e.g. &amp; → &\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))       # 😀 →  smile_face \n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)                         # mentions\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)                         # hashtags\n",
    "    text = re.sub(r\"[^\\w\\s.,!?;:]\", \"\", text)                # special chars\n",
    "    text = text.lower()                                      # lowercase\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()                 # collapse spaces\n",
    "    return text\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Create some example reviews\n",
    "# -----------------------------\n",
    "example_reviews = [\n",
    "    \"This was painfully boring. I almost fell asleep halfway through.\",\n",
    "    \"Terrible plot but the cinematography is decent.\",\n",
    "    \"It's okay overall—nothing special, nothing awful.\",\n",
    "    \"I really enjoyed the characters, solid pacing and great music!\",\n",
    "    \"Masterpiece. Best film I've seen in years.\",\n",
    "    \"Mediocre at best; some scenes work but most don't.\",\n",
    "    \"Awful acting, clumsy script. Would not recommend.\",\n",
    "    \"Charming and heartfelt with a satisfying ending.\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Load tokenizer & model\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_name)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    pretrained_name,\n",
    "    num_labels=NUM_LABELS\n",
    ")\n",
    "# Load your fine-tuned weights\n",
    "state = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Prediction helpers\n",
    "# -----------------------------\n",
    "def predict_reviews(reviews, max_length=512, batch_size=16):\n",
    "    # clean\n",
    "    texts = [clean(str(t)) for t in reviews]\n",
    "\n",
    "    all_preds, all_probs = [], []\n",
    "    # simple batching for large lists\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            preds = probs.argmax(dim=-1)\n",
    "\n",
    "        all_preds.extend(preds.detach().cpu().tolist())\n",
    "        all_probs.extend(probs.detach().cpu().tolist())\n",
    "\n",
    "    # build a results dataframe\n",
    "    top_prob = [max(p) for p in all_probs]\n",
    "    pred_label = [label_names[i] for i in all_preds]\n",
    "    df = pd.DataFrame({\n",
    "        \"review\": reviews,\n",
    "        \"review_clean\": texts,\n",
    "        \"pred_label\": pred_label,\n",
    "        \"pred_id\": all_preds,\n",
    "        \"pred_confidence\": top_prob\n",
    "    })\n",
    "    return df, all_probs\n",
    "\n",
    "def predict_one(review_text):\n",
    "    df, _ = predict_reviews([review_text])\n",
    "    row = df.iloc[0]\n",
    "    return row[\"pred_label\"], row[\"pred_confidence\"], row\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Run on the sample reviews\n",
    "# -----------------------------\n",
    "results_df, probs = predict_reviews(example_reviews)\n",
    "print(results_df[[\"review\", \"pred_label\", \"pred_confidence\"]])\n",
    "\n",
    "# (Optional) show full probability distribution per review\n",
    "for i, r in enumerate(example_reviews):\n",
    "    print(f\"\\nReview: {r}\")\n",
    "    for k, p in zip(label_names, probs[i]):\n",
    "        print(f\"  {k:>15}: {p:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7) How to use with your own text(s)\n",
    "# -----------------------------\n",
    "# Example:\n",
    "# label, conf, row = predict_one(\"The movie was outstanding, I loved every minute!\")\n",
    "# print(label, conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9add2a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review pred_label  \\\n",
      "0  I’m thrilled to report that this film redefine...   positive   \n",
      "1  On paper, this should have been my favorite re...   negative   \n",
      "2  I didn’t exactly dislike it, which isn’t to sa...   negative   \n",
      "3  Wow, what an experience 🤯—I learned exactly ho...   positive   \n",
      "4  If mediocrity were an art, this would hang in ...   negative   \n",
      "\n",
      "   pred_confidence  \n",
      "0         0.988354  \n",
      "1         0.997311  \n",
      "2         0.670189  \n",
      "3         0.896984  \n",
      "4         0.995633  \n"
     ]
    }
   ],
   "source": [
    "trick_reviews = [\n",
    "    r\"I’m thrilled to report that this film redefined cinema for me—specifically, it redefined the precise moment I should have left the theater. The opening shot is breathtaking if your breath is taken by beige hallways and unlit rooms. Performances are “committed,” in the sense that the actors should be committed for agreeing to this script. The director clearly loves long pauses; unfortunately, so did my streaming app when I tried to fast-forward. The plot twist arrives like a surprise party thrown for someone who moved away last year: nobody asked for it and the cake is stale. Still, the soundtrack works—if your playlist is “Elevator Drones Vol. 3.” Credit where due: the credits rolled eventually. I can’t wait to recommend this to people I don’t like, so we have something new not to talk about.\",\n",
    "    r\"On paper, this should have been my favorite release of the year. The lighting is meticulous, the production design has that lived-in feel, and a few lines sparkle with wit. Yet scene after scene drifts by like a well-polished boat going nowhere. I admired the craft while checking the time; I enjoyed the score while wondering why the characters sounded like they were reading stage directions. The lead’s vulnerability is disarming, but it mostly disarms tension. Even the climax feels carefully underlined in pencil—technically clear, emotionally faint. I left thinking, “That was… competent?” I wouldn’t call it bad; I’d call it an elegantly framed shrug, the kind of movie you recommend with a soft maybe and a strong caveat. If you need something pretty for a lazy Sunday, it’s fine. If you want to feel anything sharper than polite approval, you may want to look elsewhere.\",\n",
    "    r\"I didn’t exactly dislike it, which isn’t to say I liked it—more that I can’t claim I wasn’t not entertained. The pacing isn’t entirely without momentum, and the jokes aren’t completely devoid of timing, though they rarely land in a way that would be impossible to ignore. The performances don’t lack effort, but the effort doesn’t quite not show, if you follow. It’s not the kind of movie I wouldn’t avoid recommending to someone who’s not uninterested in background noise. By the time the not-unexpected ending arrived, I was neither unmoved nor moved, just not unready for the credits. In fairness, the cinematography isn’t unappealing, and the score isn’t unlistenable; both do a job that isn’t unneeded. I suppose calling it “not terrible” wouldn’t be inaccurate, as long as “not good” doesn’t feel unkind. If ambiguity was the goal, mission… not unaccomplished.\",\n",
    "    r\"Wow, what an experience 🤯—I learned exactly how long ninety minutes can feel when time stops. The trailer promised thrills 😍, but the movie delivered responsible seatbelt usage and a deep respect for indoor voices. The romance is “simmering” if by simmering you mean two strangers who occasionally nod. I loved the soundtrack in the sense that silence is technically a sound 👂. Credit to the editor for keeping every scene long enough to wonder if my remote died. There’s a twist ending 🎁, which is adorable—like when a toddler hides behind a curtain and says, “Guess where I am!” Still, I’m grateful: the closing credits gave me the year’s most honest character arc—me, leaving. If you crave excitement, try watching paint dry, then watch this to cool down. Five stars for existing ⭐ (calm down, algorithm, that’s sarcasm), zero for joy.\",\n",
    "    r\"If mediocrity were an art, this would hang in a tasteful gallery next to a plaque that reads, “Adequate, 2025.” Compared to the director’s earlier disaster, it’s practically a comeback; compared to anything genuinely good, it’s a carefully ironed bedsheet—smooth, flat, and forgettable. The performances are fine in that way airplanes are fine: you arrive, you don’t applaud. I admired how the script avoids clichés by replacing them with footnotes, each scene explaining why it exists instead of earning the right to. Yet I can’t deny a strange comfort: it’s competent, even considerate, never insulting my intelligence so much as politely ignoring it. Tomorrow I’ll remember a line or two; next week I’ll struggle to recall the title. Would I watch it again? Only if I’ve already seen everything else and need something to not mind missing.\"\n",
    "    ]\n",
    "results_df, probs = predict_reviews(trick_reviews)\n",
    "print(results_df[[\"review\", \"pred_label\", \"pred_confidence\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
